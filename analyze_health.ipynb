{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "DATA_DIR = \"my_whoop_data_2026_02_11\"\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        journal = pd.read_csv(os.path.join(DATA_DIR, \"journal_entries.csv\"))\n",
    "        phys = pd.read_csv(os.path.join(DATA_DIR, \"physiological_cycles.csv\"))\n",
    "        sleeps = pd.read_csv(os.path.join(DATA_DIR, \"sleeps.csv\"))\n",
    "        workouts = pd.read_csv(os.path.join(DATA_DIR, \"workouts.csv\"))\n",
    "        happiness = pd.read_csv(os.path.join(DATA_DIR, \"subjective_happiness.csv\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading files: {e}\")\n",
    "        return None, None, None, None, None\n",
    "    return journal, phys, sleeps, workouts, happiness\n",
    "\n",
    "def process_journal(journal_df):\n",
    "    print(\"Processing Journal Entries...\")\n",
    "    journal_df['Cycle start time'] = pd.to_datetime(journal_df['Cycle start time'])\n",
    "    journal_df = journal_df.drop_duplicates(subset=['Cycle start time', 'Question text'])\n",
    "    journal_df['Answered yes'] = journal_df['Answered yes'].astype(str).str.lower().map({'true': 1, 'false': 0})\n",
    "    pivoted = journal_df.pivot(index='Cycle start time', columns='Question text', values='Answered yes')\n",
    "    return pivoted\n",
    "\n",
    "def process_physiological(phys_df):\n",
    "    print(\"Processing Physiological Cycles...\")\n",
    "    phys_df['Cycle start time'] = pd.to_datetime(phys_df['Cycle start time'])\n",
    "    phys_df['Wake onset'] = pd.to_datetime(phys_df['Wake onset'])\n",
    "    phys_df['Date'] = phys_df['Wake onset'].dt.normalize()\n",
    "    cols = ['Cycle start time', 'Date', 'Recovery score %', 'Resting heart rate (bpm)', 'Heart rate variability (ms)', 'Day Strain']\n",
    "    cols = [c for c in cols if c in phys_df.columns]\n",
    "    return phys_df[cols]\n",
    "\n",
    "def process_sleeps(sleeps_df):\n",
    "    print(\"Processing Sleeps...\")\n",
    "    sleeps_df['Cycle start time'] = pd.to_datetime(sleeps_df['Cycle start time'])\n",
    "    cols = ['Cycle start time', 'Sleep efficiency %', 'Sleep performance %']\n",
    "    return sleeps_df[cols]\n",
    "\n",
    "def process_workouts(workouts_df):\n",
    "    print(\"Processing Workouts...\")\n",
    "    workouts_df['Workout start time'] = pd.to_datetime(workouts_df['Workout start time'])\n",
    "    workouts_df['Workout Date'] = workouts_df['Workout start time'].dt.normalize()\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        col_pct = f'HR Zone {i} %'\n",
    "        col_min = f'Zone {i} Minutes'\n",
    "        if col_pct in workouts_df.columns:\n",
    "            workouts_df[col_min] = workouts_df['Duration (min)'] * (workouts_df[col_pct] / 100.0)\n",
    "    \n",
    "    agg_funcs = {'Activity Strain': 'sum', 'Duration (min)': 'sum'}\n",
    "    for i in range(1, 6):\n",
    "        col_min = f'Zone {i} Minutes'\n",
    "        if col_min in workouts_df.columns:\n",
    "            agg_funcs[col_min] = 'sum'\n",
    "            \n",
    "    aggregated = workouts_df.groupby('Workout Date').agg(agg_funcs).reset_index()\n",
    "    # Shift Date: Workout Date (Day N-1) -> Affects Recovery Date (Day N)\n",
    "    aggregated['Date'] = aggregated['Workout Date'] + pd.Timedelta(days=1)\n",
    "    return aggregated\n",
    "\n",
    "def process_happiness(happiness_df):\n",
    "    print(\"Processing Happiness...\")\n",
    "    happiness_df['Date'] = pd.to_datetime(happiness_df['Date']).dt.normalize()\n",
    "    return happiness_df\n",
    "\n",
    "def train_recovery_model(df):\n",
    "    print(\"\\n--- Training Deep Learning Model for Strain Optimization ---\")\n",
    "    \n",
    "    # Sort by date to ensure lags are correct\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # We want to predict Next Day Recovery (N+1)\n",
    "    # Using: Recovery(N), Sleep(N), Strain(N), Strain(N-1)...Strain(N-7)\n",
    "    \n",
    "    # Target: Next Day Recovery\n",
    "    # Note: 'Recovery score %' in `df` is already for Day N (merged on Date).\n",
    "    # So we want to predict `df['Recovery score %'].shift(-1)`\n",
    "    df['Target_Next_Recovery'] = df['Recovery score %'].shift(-1)\n",
    "    \n",
    "    # Features\n",
    "    # Current Day N features\n",
    "    # features = ['Recovery score %', 'Sleep efficiency %', 'Activity Strain'] # REMOVED to avoid duplication\n",
    "    features = []\n",
    "    \n",
    "    # Past Strains: Strain(N) is 'Activity Strain'.\n",
    "    # We need Strain(N-1) ... Strain(N-7).\n",
    "    # However, 'Activity Strain' in `df` is ALREADY shifted in `process_workouts`\n",
    "    # In `process_workouts`: `Date` = `Workout Date` + 1.\n",
    "    # So `df.loc[Date=T, 'Activity Strain']` is actually the strain performed on T-1.\n",
    "    \n",
    "    # Let's clarify the timeline:\n",
    "    # Date T: We wake up, measure Recovery(T). We sleep T-1 -> T.\n",
    "    # The 'Activity Strain' merged on Date T corresponds to workout on T-1.\n",
    "    \n",
    "    # Optimization Goal:\n",
    "    # We are at Date T (morning). We have Recovery(T).\n",
    "    # We want to decide Strain(T).\n",
    "    # This Strain(T) will affect Recovery(T+1).\n",
    "    \n",
    "    # So the model should predict Recovery(T+1).\n",
    "    # Inputs:\n",
    "    # - Recovery(T) (Morning state)\n",
    "    # - Strain(T) (The decision variable - workout we will do today)\n",
    "    # - Strain(T-1) (Yesterday's workout - already in `df` at Date T)\n",
    "    # - Strain(T-2)... (Fatigue history)\n",
    "    \n",
    "    # In `df`, the row for Date T contains:\n",
    "    # - Recovery(T)\n",
    "    # - Activity Strain (Workout on T-1)\n",
    "    \n",
    "    # So to get Strain(T) as a feature for Row T, we actually need `Activity Strain` from Row T+1.\n",
    "    # But Row T+1 doesn't exist yet when we are predicting.\n",
    "    # So we are building a model where:\n",
    "    # Row T features:\n",
    "    # - Recovery(T)\n",
    "    # - Strain(T) (Future variable relative to T's row data, but we can shift it back)\n",
    "    # - Strain(T-1) (Current row's 'Activity Strain')\n",
    "    # - Strain(T-2) (Lag 1 of 'Activity Strain')\n",
    "    # ...\n",
    "    \n",
    "    # Let's realign features to be intuitive:\n",
    "    # Let's Create 'Proposed_Strain_Today' which is `Activity Strain` shifted by -1 (from T+1 to T).\n",
    "    df['Proposed_Strain_Today'] = df['Activity Strain'].shift(-1)\n",
    "    \n",
    "    # Create Lag features for history (Strain T-1, T-2...)\n",
    "    # Strain T-1 is `Activity Strain` at row T.\n",
    "    # Strain T-2 is `Activity Strain` at row T shifted +1 (Lag 1)\n",
    "    for i in range(1, 8):\n",
    "        df[f'Strain_Lag_{i}'] = df['Activity Strain'].shift(i-1) \n",
    "        features.append(f'Strain_Lag_{i}')\n",
    "    \n",
    "    # Add Proposed_Strain_Today to features\n",
    "    features = ['Recovery score %', 'Sleep efficiency %', 'Proposed_Strain_Today'] + features\n",
    "    \n",
    "    # Drop NaN rows created by shifting\n",
    "    model_df = df.dropna(subset=['Target_Next_Recovery', 'Proposed_Strain_Today'] + features)\n",
    "    \n",
    "    if len(model_df) < 50:\n",
    "        print(\"Not enough data points for Deep Learning (<50). Skipping.\")\n",
    "        return None, None, features\n",
    "        \n",
    "    X = model_df[features].values\n",
    "    y = model_df['Target_Next_Recovery'].values\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define Model\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(len(features),)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['mae'])\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training on {len(X_train)} samples...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Model Trained. Test MAE: {mae:.2f}% (Average Error in Recovery Score prediction)\")\n",
    "    \n",
    "    return model, scaler, features\n",
    "\n",
    "def predict_optimal_strain(model, scaler, feature_names, df):\n",
    "    if model is None:\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Optimizing Strain for Tomorrow ---\")\n",
    "    \n",
    "    # Sort and clean\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Get the most recent valid day's data (State T)\n",
    "    # We need Recovery(T) and Sleep(T) to be present.\n",
    "    valid_df = df.dropna(subset=['Recovery score %', 'Sleep efficiency %'])\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        print(\"No valid data found for prediction.\")\n",
    "        return\n",
    "\n",
    "    last_row = valid_df.iloc[-1]\n",
    "    \n",
    "    # Prepare base features\n",
    "    # Recovery(T)\n",
    "    rec_t = last_row['Recovery score %']\n",
    "    # Sleep(T) (Assume available, or use mean if missing/future)\n",
    "    # For simulation, we use the value from the last row (Last night's sleep)\n",
    "    sleep_t = last_row['Sleep efficiency %']\n",
    "    \n",
    "    # Strain History\n",
    "    # Strain(T-1) is `Activity Strain` in last_row\n",
    "    strain_t_minus_1 = last_row['Activity Strain']\n",
    "    \n",
    "    # We need Strains T-2...T-7\n",
    "    # Get history prior to current date\n",
    "    current_date = last_row['Date']\n",
    "    history_df = df[df['Date'] < current_date].sort_values('Date')\n",
    "    \n",
    "    # Get last 6 strains (T-2 to T-7)\n",
    "    past_strains = history_df['Activity Strain'].tail(6).values[::-1]\n",
    "    \n",
    "    # Combine T-1 and T-2...T-7\n",
    "    strain_history = np.concatenate(([strain_t_minus_1], past_strains))\n",
    "    \n",
    "    # Pad if not enough history\n",
    "    if len(strain_history) < 7:\n",
    "        strain_history = np.pad(strain_history, (0, 7-len(strain_history)), 'constant')\n",
    "    \n",
    "    # Loop to find optimal Strain(T)\n",
    "    print(f\"Current State (Day N): Recovery={rec_t}%, Sleep={sleep_t}%, Prev Strain={strain_t_minus_1}\")\n",
    "    print(\"Simulating Strain(N) from 0 to 21...\")\n",
    "    \n",
    "    best_strain = 0\n",
    "    max_rec = 0\n",
    "    found_target = False\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for strain_val in np.arange(0, 21.5, 0.5):\n",
    "        # Construct Input Vector\n",
    "        # Features order: ['Recovery score %', 'Sleep efficiency %', 'Proposed_Strain_Today', 'Strain_Lag_1'...'Strain_Lag_7']\n",
    "        # Note: 'Strain_Lag_1' corresponds to Strain(T-1)\n",
    "        \n",
    "        input_vec = [rec_t, sleep_t, strain_val]\n",
    "        # Add history (T-1 to T-7)\n",
    "        input_vec.extend(strain_history[:7])\n",
    "        \n",
    "        # Reshape and Scale\n",
    "        input_vec = np.array(input_vec).reshape(1, -1)\n",
    "        input_vec_scaled = scaler.transform(input_vec)\n",
    "        \n",
    "        # Predict\n",
    "        pred_recovery = model.predict(input_vec_scaled, verbose=0)[0][0]\n",
    "        results.append((strain_val, pred_recovery))\n",
    "        \n",
    "        if pred_recovery > max_rec:\n",
    "            max_rec = pred_recovery\n",
    "            best_strain = strain_val\n",
    "            \n",
    "        if pred_recovery >= 67:\n",
    "            print(f\" -> Found Optimal Strain: {strain_val} (Predicted Recovery: {pred_recovery:.1f}%)\")\n",
    "            best_strain = strain_val\n",
    "            max_rec = pred_recovery\n",
    "            found_target = True\n",
    "            # We want the HIGHEST strain that keeps us above 67?\n",
    "            # Or just ANY? \"Optimize for strain so that my recovery is at least 67%\"\n",
    "            # This usually means \"Maximize Strain subject to Recovery >= 67\".\n",
    "            # So we should continue loop and keep updating best_strain as long as cond is met.\n",
    "            \n",
    "    if found_target:\n",
    "        # Filter for all valid strains\n",
    "        valid_strains = [s for s, r in results if r >= 67]\n",
    "        optimal_s = max(valid_strains)\n",
    "        pred_r = [r for s, r in results if s == optimal_s][0]\n",
    "        print(f\"\\n*** RECOMMENDATION ***\")\n",
    "        print(f\"To achieve >= 67% Recovery tomorrow, you can go up to Strain: {optimal_s}\")\n",
    "        print(f\"Predicted Recovery: {pred_r:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n*** WARNING ***\")\n",
    "        print(f\"Even with 0 Strain, predicted recovery is only {max_rec:.1f}%.\")\n",
    "        print(\"Recommendation: Rest Day (Strain 0).\")\n",
    "\n",
    "def main():\n",
    "    journal, phys, sleeps, workouts, happiness = load_data()\n",
    "    if journal is None:\n",
    "        return\n",
    "\n",
    "    # 1. Process Main Data (Phys + Sleep + Journal)\n",
    "    phys_proc = process_physiological(phys)\n",
    "    sleeps_proc = process_sleeps(sleeps)\n",
    "    journal_proc = process_journal(journal)\n",
    "    \n",
    "    # Merge Phys and Sleep\n",
    "    main_df = pd.merge(phys_proc, sleeps_proc, on='Cycle start time', how='left')\n",
    "    \n",
    "    # Merge Journal\n",
    "    main_df = pd.merge(main_df, journal_proc, left_on='Cycle start time', right_index=True, how='left')\n",
    "    \n",
    "    # 2. Process Workouts\n",
    "    workouts_proc = process_workouts(workouts)\n",
    "    \n",
    "    # 3. Process Happiness\n",
    "    happiness_proc = process_happiness(happiness)\n",
    "    \n",
    "    # 4. Final Merge on Date\n",
    "    full_df = pd.merge(main_df, workouts_proc, on='Date', how='left')\n",
    "    \n",
    "    # Fill NaN for workout columns with 0\n",
    "    workout_cols = ['Activity Strain', 'Duration (min)'] + [c for c in workouts_proc.columns if 'Zone' in c and 'Minutes' in c]\n",
    "    workout_cols = [c for c in workout_cols if c in full_df.columns]\n",
    "    full_df[workout_cols] = full_df[workout_cols].fillna(0)\n",
    "    print(f\"Filled NaN values with 0 for {len(workout_cols)} workout columns (rest days).\")\n",
    "\n",
    "    # Merge Happiness\n",
    "    full_df = pd.merge(full_df, happiness_proc, on='Date', how='left')\n",
    "    \n",
    "    # 5. Correlation Analysis\n",
    "    numeric_df = full_df.select_dtypes(include=['number'])\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    \n",
    "    target = 'Recovery score %'\n",
    "    if target in corr_matrix.columns:\n",
    "        print(f\"\\n--- Top 5 Positive Influences on {target} ---\")\n",
    "        correlations = corr_matrix[target].drop(target)\n",
    "        print(correlations.sort_values(ascending=False).head(5))\n",
    "        \n",
    "        print(f\"\\n--- Top 5 Negative Influences on {target} ---\")\n",
    "        print(correlations.sort_values(ascending=True).head(5))\n",
    "\n",
    "    # 6. Heatmap\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title(\"Correlation Matrix: Behaviors, Workouts, and Recovery\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"correlation_heatmap.png\")\n",
    "    print(\"\\nHeatmap saved to correlation_heatmap.png\")\n",
    "    \n",
    "    # 7. Deep Learning & Optimization\n",
    "    # We pass full_df. Copy it to avoid setting with copy warnings\n",
    "    dl_df = full_df.copy()\n",
    "    model, scaler, feature_names = train_recovery_model(dl_df)\n",
    "    \n",
    "    if model:\n",
    "        predict_optimal_strain(model, scaler, feature_names, dl_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
